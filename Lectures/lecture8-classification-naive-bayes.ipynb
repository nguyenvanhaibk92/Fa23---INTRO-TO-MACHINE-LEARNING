{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "editable": true,
    "id": "i_f5u2x9nn6I",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Lecture 8: Naive Bayes Classification\n",
    "\n",
    "Adapted from Applied Machine Learning Lecture Notes of Volodymyr Kuleshov, Cornel Tech\n",
    "\n",
    "__Instructor Tan Bui__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Part 1: Text Classification\n",
    "\n",
    "We will now do a quick detour to talk about an important application area of machine learning: text classification. \n",
    "\n",
    "Afterwards, we will see how text classification motivates new classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Review: Classification\n",
    "\n",
    "Consider a training dataset $\\mathcal{D} = \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots, (x^{(n)}, y^{(n)})\\}$.\n",
    "\n",
    "We distinguish between two types of supervised learning problems depnding on the targets $y^{(i)}$. \n",
    "\n",
    "1. __Regression__: The target variable $y \\in \\mathcal{Y}$ is continuous:  $\\mathcal{Y} \\subseteq \\mathbb{R}$.\n",
    "2. __Classification__: The target variable $y$ is discrete and takes on one of $K$ possible values:  $\\mathcal{Y} = \\{y_1, y_2, \\ldots y_K\\}$. Each discrete value corresponds to a *class* that we want to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Text Classification\n",
    "\n",
    "An interesting instance of a classification problem is classifying text.\n",
    "* Includes a lot applied problems: spam filtering, fraud detection, medical record classification, etc.\n",
    "* Inputs $x$ are sequences of words of an arbitrary length.\n",
    "* The dimensionality of text inputs is usually very large, proportional to the size of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Classification Dataset: Twenty Newsgroups\n",
    "\n",
    "To illustrate the text classification problem, we will use a popular dataset called `20-newsgroups`. \n",
    "* It contains ~20,000 documents collected approximately evenly from 20 different online newsgroups.\n",
    "* Each newgroup covers a different topic such as medicine, computer graphics, or religion.\n",
    "* This dataset is widely used to benchmark text classification and other types of algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Let's load this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _20newsgroups_dataset:\n",
      "\n",
      "The 20 newsgroups text dataset\n",
      "------------------------------\n",
      "\n",
      "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
      "20 topics split in two subsets: one for training (or development)\n",
      "and the other one for testing (or for performance evaluation). The split\n",
      "between the train and test set is based upon a messages posted before\n",
      "and after a specific date.\n",
      "\n",
      "This module contains two loaders. The first one,\n",
      ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
      "returns a list of the raw texts that can be fed to text feature\n",
      "extractors such as :class:`~sklearn.feature_extraction.text.CountVectorizer`\n",
      "with custom parameters so as to extract feature vectors.\n",
      "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
      "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
      "extractor.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   ==========\n",
      "    Classes                     20\n",
      "    Samples total            18846\n",
      "    Dimensionality               1\n",
      "    Features                  text\n",
      "    =================   ==========\n",
      "\n",
      "Usage\n",
      "~~~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# for this lecture, we will restrict our attention to just 4 different newsgroups:\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "\n",
    "# load the dataset\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "# print some information on it\n",
    "print(twenty_train.DESCR[:1100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The set of targets in this dataset are the newgroup topics:\n",
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: s0612596@let.rug.nl (M.M. Zwart)\n",
      "Subject: catholic church poland\n",
      "Organization: Faculteit der Letteren, Rijksuniversiteit Groningen, NL\n",
      "Lines: 10\n",
      "\n",
      "Hello,\n",
      "\n",
      "I'm writing a paper on the role of the catholic church in Poland after 1989. \n",
      "Can anyone tell me more about this, or fill me in on recent books/articles(\n",
      "in english, german or french). Most important for me is the role of the \n",
      "church concerning the abortion-law, religious education at schools,\n",
      "birth-control and the relation church-state(government). Thanx,\n",
      "\n",
      "                                                 Masja,\n",
      "\"M.M.Zwart\"<s0612596@let.rug.nl>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's examine one data point\n",
    "print(twenty_train.data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2257\n"
     ]
    }
   ],
   "source": [
    "# We have about 2k data points in total\n",
    "print(len(twenty_train.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Feature Representations for Text\n",
    "\n",
    "Each data point $x$ in this dataset is a squence of characters of an arbitrary length.\n",
    "\n",
    "How do we transform these into $d$-dimensional features $\\phi(x)$ that can be used with our machine learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* We may devise hand-crafted features by inspecting the data:\n",
    "    * Does the message contain the word \"church\"? Does the email of the user originate outside the United States? Is the organization a university? etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* We can count the number of occurrences of each word:\n",
    "    * Does this message contain \"Aardvark\", yes or no?\n",
    "    * Does this message contain \"Apple\", yes or no?\n",
    "    * ... Does this message contain \"Zebra\", yes or no?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Finally, many modern deep learning methods can directly work with sequences of characters of an arbitrary length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Bag of Words Representations\n",
    "\n",
    "Perhaps the most widely used approach to representing text documents is called \"bag of words\".\n",
    "\n",
    "We start by defining a vocabulary $V$ containing all the possible words we are interested in, e.g.:\n",
    "$$ V = \\{\\text{church}, \\text{doctor}, \\text{fervently}, \\text{purple}, \\text{slow}, ...\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "A bag of words representation of a document $x$ is a function $\\phi(x) \\to \\{0,1\\}^{|V|}$ that outputs a feature vector\n",
    "$$\n",
    "\\phi(x) = \\left( \n",
    "\\begin{array}{c}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\begin{array}{l}\n",
    "\\;\\text{church} \\\\\n",
    "\\;\\text{doctor} \\\\\n",
    "\\;\\text{fervently} \\\\\n",
    "\\\\\n",
    "\\;\\text{purple} \\\\\n",
    "\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "of dimension $|V|$. The $j$-th component $\\phi(x)_j$ equals $1$ if $x$ convains the $j$-th word in $V$ and $0$ otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Let's see an example of this approach on `20-newsgroups`.\n",
    "\n",
    "We start by computing these features using the `sklearn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vectorize the training set\n",
    "count_vect = CountVectorizer(binary=True)\n",
    "X_train = count_vect.fit_transform(twenty_train.data)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "In `sklearn`, we can retrieve the index of $\\phi(x)$ associated with each `word` using the expression `count_vect.vocabulary_.get(word)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index for the word \"church\":  8609\n",
      "Index for the word \"computer\":  9338\n"
     ]
    }
   ],
   "source": [
    "# The CountVectorizer class records the index j associated with each word in V\n",
    "print('Index for the word \"church\": ', count_vect.vocabulary_.get(u'church'))\n",
    "print('Index for the word \"computer\": ', count_vect.vocabulary_.get(u'computer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Our featurized dataset is in the matrix `X_train`. We can use the above indices to retrieve the 0-1 value that has been computed for each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: s0612596@let.rug.nl (M.M. Zwart)\n",
      "Subject: catholic church poland\n",
      "Organization: Faculteit der Letteren, Rijksuniversiteit Groningen, NL\n",
      "Lines: 10\n",
      "\n",
      "Hello,\n",
      "\n",
      "I'm writing a paper on the role of the catholic church in Poland after 1989. \n",
      "Can anyone tell me more about this, or fill me in on recent books/articles(\n",
      "in english, german or french). Most important for me is the role of the \n",
      "church concerning the abortion-law, religious education at schools,\n",
      "birth-control and the relation church-state(government). Thanx,\n",
      "\n",
      "                                                 Masja,\n",
      "\"M.M.Zwart\"<s0612596@let.rug.nl>\n",
      "\n",
      "------------------------------------------------------------\n",
      "Value at the index for the word \"church\":  1\n",
      "Value at the index for the word \"computer\":  0\n",
      "Value at the index for the word \"doctor\":  0\n",
      "Value at the index for the word \"important\":  1\n"
     ]
    }
   ],
   "source": [
    "# We can examine if any of these words are present in our previous datapoint\n",
    "print(twenty_train.data[3])\n",
    "\n",
    "# let's see if it contains these two words?\n",
    "print('---'*20)\n",
    "print('Value at the index for the word \"church\": ', X_train[3, count_vect.vocabulary_.get(u'church')])\n",
    "print('Value at the index for the word \"computer\": ', X_train[3, count_vect.vocabulary_.get(u'computer')])\n",
    "print('Value at the index for the word \"doctor\": ', X_train[3, count_vect.vocabulary_.get(u'doctor')])\n",
    "print('Value at the index for the word \"important\": ', X_train[3, count_vect.vocabulary_.get(u'important')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Practical Considerations\n",
    "\n",
    "In practice, we may use some additional modifications of this techinque:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Sometimes, the feature $\\phi(x)_j$ for the $j$-th word holds the count of occurrences of word $j$ instead of just the binary occurrence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* The raw text is usually preprocessed. One common technique is *stemming*, in which we only keep the root of the word.\n",
    "    * e.g. \"slowly\", \"slowness\", both map to \"slow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Filtering for common *stopwords* such as \"the\", \"a\", \"and\". Similarly, rare words are also typically excluded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Classification Using BoW Features\n",
    "\n",
    "Let's now have a look at the performance of classification over bag of words features.\n",
    "\n",
    "Now that we have a feature representation $\\phi(x)$, we can apply the classifier of our choice, such as logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, multi_class='multinomial', verbose=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create an instance of Softmax and fit the data.\n",
    "logreg = LogisticRegression(C=1e5, multi_class='multinomial', verbose=True)\n",
    "logreg.fit(X_train, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "And now we can use this model for predicting on new inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "\n",
    "X_new = count_vect.transform(docs_new)\n",
    "predicted = logreg.predict(X_new)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Summary of Text Classification\n",
    "\n",
    "* Classifying text normally requires specifiyng features over the raw data.\n",
    "* A widely used representation is \"bag of words\", in which features are occurrences or counts of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Once text is featurized, any off-the-shelf supervised learning algorithm can be applied, but some work better than others, as we will see next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Part 2: Naive Bayes\n",
    "\n",
    "Next, we are going to look at Naive Bayes --- a generative classification algorithm. We will apply Naive Bayes to the text classification problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "# Review: Classification\n",
    "\n",
    "Consider a training dataset $\\mathcal{D} = \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots, (x^{(n)}, y^{(n)})\\}$.\n",
    "\n",
    "We distinguish between two types of supervised learning problems depnding on the targets $y^{(i)}$. \n",
    "\n",
    "1. __Regression__: The target variable $y \\in \\mathcal{Y}$ is continuous:  $\\mathcal{Y} \\subseteq \\mathbb{R}$.\n",
    "2. __Classification__: The target variable $y$ is discrete and takes on one of $K$ possible values:  $\\mathcal{Y} = \\{y_1, y_2, \\ldots y_K\\}$. Each discrete value corresponds to a *class* that we want to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "# Review: Text Classification\n",
    "\n",
    "An interesting instance of a classification problem is classifying text.\n",
    "* Includes a lot applied problems: spam filtering, fraud detection, medical record classification, etc.\n",
    "* Inputs $x$ are sequences of words of an arbitrary length.\n",
    "* The dimensionality of text inputs is usually very large, proportional to the size of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Review: Bag of Words Features\n",
    "\n",
    "Given a vocabulary $V$, a bag of words representation of a document $x$ is a function $\\phi(x) \\to \\{0,1\\}^{|V|}$ that outputs a feature vector\n",
    "$$\n",
    "\\phi(x) = \\left( \n",
    "\\begin{array}{c}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\begin{array}{l}\n",
    "\\;\\text{church} \\\\\n",
    "\\;\\text{doctor} \\\\\n",
    "\\;\\text{fervently} \\\\\n",
    "\\\\\n",
    "\\;\\text{purple} \\\\\n",
    "\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "of dimension $|V|$. The $j$-th component $\\phi(x)_j$ equals $1$ if $x$ convains the $j$-th word in $V$ and $0$ otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Review: Generative Models\n",
    "\n",
    "There are two types of probabilistic models: *generative* and *discriminative*.\n",
    "\\begin{align*}\n",
    "\\underbrace{P_\\theta(x,y) : \\mathcal{X} \\times \\mathcal{Y} \\to [0,1]}_\\text{generative model} & \\;\\; & \\underbrace{P_\\theta(y|x) : \\mathcal{X} \\times \\mathcal{Y} \\to [0,1]}_\\text{discriminative model}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "For generative approach, given a new datapoint $x'$, we can match it against each class model and find the class that looks most similar to it:\n",
    "\\begin{align*}\n",
    "\\arg \\max_y \\log p(y | x) = \\arg \\max_y \\log \\frac{p(x | y) p(y)}{p(x)} = \\arg \\max_y \\log p(x | y) p(y),\n",
    "\\end{align*}\n",
    "where we have applied Bayes' rule in the second equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Review: Gaussian Discriminant Model\n",
    "\n",
    "The GDA algorithm defines the following model family.\n",
    "* The probability $P(x\\mid y=k)$ of the data under class $k$ is a [multivariate Gaussian](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) $\\mathcal{N}(x; \\mu_k, \\Sigma_k)$ with parameters\n",
    "$\\mu_k, \\Sigma_k$.\n",
    "* The distribution over classes is [Categorical](https://en.wikipedia.org/wiki/Categorical_distribution), denoted $\\text{Categorical}(\\phi_1, \\phi_2, ..., \\phi_K)$. Thus, $P_\\theta(y=k) = \\phi_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Thus, $P_\\theta(x)$ is a mixture of $K$ Gaussians:\n",
    "$$P_\\theta(x) = \\sum_{k=1}^K P_\\theta(y=k) P_\\theta(x|y=k) = \\sum_{k=1}^K \\phi_k \\mathcal{N}(x; \\mu_k, \\Sigma_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Problem 1: Discrete Data\n",
    "\n",
    "What would happen if we used GDA to perform text classification?\n",
    "The first problem we face is that the input data is discrete:\n",
    "$$\n",
    "\\phi(x) = \\left( \n",
    "\\begin{array}{c}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\begin{array}{l}\n",
    "\\;\\text{church} \\\\\n",
    "\\;\\text{doctor} \\\\\n",
    "\\;\\text{fervently} \\\\\n",
    "\\\\\n",
    "\\;\\text{purple} \\\\\n",
    "\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "This data does not follows a Normal distribution, hence the GDA model is clearly misspecified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Problem 2: High Dimensionality\n",
    "\n",
    "A first solution is to assume that $x$ is sampled from a categorical distribution that **assigns a probability to each possible state of $x$**.\n",
    "$$\n",
    "p(x) = p \\left( \n",
    "\\begin{array}{c}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0 \n",
    "\\end{array}\n",
    "\\right.\n",
    "\\left.\n",
    "\\begin{array}{l}\n",
    "\\;\\text{church} \\\\\n",
    "\\;\\text{doctor} \\\\\n",
    "\\;\\text{fervently} \\\\\n",
    "\\vdots \\\\\n",
    "\\;\\text{purple}\n",
    "\\end{array}\n",
    "\\right) = 0.0012\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "However, if the dimensionality $d$ of $x$ is high (e.g., vocabulary has size 10,000), $x$ can take a huge number of values ($2^{10000}$ in our example). Since the sum of probabiity is 1, we need to specify $2^{d}-1$ parameters for the categorical distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Naive Bayes Assumption\n",
    "\n",
    "In order to deal with high-dimensional $x$, we simplify the problem by making the *Naive Bayes* assumption:\n",
    "$$ p(x|y) = \\prod_{j=1}^d p(x_j \\mid y) $$\n",
    "In other words, the probability $p(x|y)$ factorizes over each dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* For example, if $x$ is a binary bag of words representation, then $p(x_j | y)$ is the probability of seeing the $j$-th word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* We can model each $p(x_j | y)$ via a Bernoulli distribution, which has only one parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Hence, it takes only $d$ parameters (instead of $2^d-1$) to specify the entire distribution $p(x|y) = \\prod_{j=1}^d p(x_j \\mid y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Bernoulli Naive Bayes Model\n",
    "\n",
    "We can apply the Naive Bayes assumption to obtain a model for when $x$ is in a bag of words representation.\n",
    "\n",
    "The *Bernoulli Naive Bayes* model $P_\\theta(x)$ is defined as follows:\n",
    "* The distribution over classes is [Categorical](https://en.wikipedia.org/wiki/Categorical_distribution), denoted $\\text{Categorical}(\\phi_1, \\phi_2, ..., \\phi_K)$. Thus, $P_\\theta(y=k) = \\phi_k$.\n",
    "* The conditional probability of the data under class $k$ factorizes as $P_\\theta(x|y=k) = \\prod_{j=1}^d P(x_j \\mid y=k)$ (the Naive Bayes assumption), where each $P_\\theta(x_j \\mid y=k)$ is a $\\text{Bernoulli}(\\psi_{jk})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Formally, for binary bag of words presentation, we have:\n",
    "\\begin{align*}\n",
    "P_\\theta(y) & = \\text{Categorical}(\\phi_1,\\phi_2,\\ldots,\\phi_K) \\\\\n",
    "P_\\theta(x_j=1|y=k) & = \\text{Bernoulli}(\\psi_{jk}) \\\\\n",
    "P_\\theta(x|y=k) & = \\prod_{j=1}^d P_\\theta(x_j|y=k)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Part 3: Naive Bayes: Learning\n",
    "\n",
    "We are going to continue our discussion of Naive Bayes.\n",
    "\n",
    "We will now turn our attention to learning the parameters of the model and using them to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "# Review: Text Classification\n",
    "\n",
    "An interesting instance of a classification problem is classifying text.\n",
    "* Includes a lot applied problems: spam filtering, fraud detection, medical record classification, etc.\n",
    "* Inputs $x$ are sequences of words of an arbitrary length.\n",
    "* The dimensionality of text inputs is usually very large, proportional to the size of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "# Review: Bag of Words Features\n",
    "\n",
    "Given a vocabulary $V$, a bag of words representation of a document $x$ is a function $\\phi(x) \\to \\{0,1\\}^{|V|}$ that outputs a feature vector\n",
    "$$\n",
    "\\phi(x) = \\left( \n",
    "\\begin{array}{c}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\begin{array}{l}\n",
    "\\;\\text{church} \\\\\n",
    "\\;\\text{doctor} \\\\\n",
    "\\;\\text{fervently} \\\\\n",
    "\\\\\n",
    "\\;\\text{purple} \\\\\n",
    "\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "of dimension $|V|$. The $j$-th component $\\phi(x)_j$ equals $1$ if $x$ convains the $j$-th word in $V$ and $0$ otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Bernoulli Naive Bayes Model\n",
    "\n",
    "The *Bernoulli Naive Bayes* model $P_\\theta(x)$ is defined as follows:\n",
    "* The distribution over classes is [Categorical](https://en.wikipedia.org/wiki/Categorical_distribution), denoted $\\text{Categorical}(\\phi_1, \\phi_2, ..., \\phi_K)$. Thus, $P_\\theta(y=k) = \\phi_k$.\n",
    "* The conditional probability of the data under class $k$ factorizes as $P_\\theta(x|y=k) = \\prod_{j=1}^d P(x_j \\mid y=k)$ (the Naive Bayes assumption), where each $P_\\theta(x_j \\mid y=k)$ is a $\\text{Bernoulli}(\\psi_{jk})$, that is, $P_\\theta(x_j = 1 \\mid y=k) = \\psi_{jk}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Review: Maximum Likelihood Learning\n",
    "\n",
    "In order to fit probabilistic models, we use the following objective:\n",
    "$$ \\max_\\theta \\mathbb{E}_{x, y \\sim \\mathbb{P}_\\text{data}} \\log P_\\theta(x, y). $$\n",
    "This seeks to find a model that assigns high probability to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Let's use maximum likelihood to fit the Bernoulli Naive Bayes model. Note that model parameterss $\\theta$ are the union of the parameters of each sub-model:\n",
    "$$\\theta = (\\phi_1, \\phi_2,\\ldots, \\phi_K, \\psi_{11}, \\psi_{21}, \\ldots, \\psi_{dK}).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Learning a Bernoulli Naive Bayes Model\n",
    "\n",
    "Given a dataset $\\mathcal{D} = \\{(x^{(i)}, y^{(i)})\\mid i=1,2,\\ldots,n\\}$, we want to optimize the log-likelihood $\\ell(\\theta) = \\log L(\\theta)$:\n",
    "\\begin{align*}\n",
    "\\ell(\\theta) & = \\sum_{i=1}^n \\log P_\\theta(x^{(i)}, y^{(i)}) = \\sum_{i=1}^n \\log P_\\theta(x^{(i)} | y^{(i)}) + \\sum_{i=1}^n \\log P_\\theta(y^{(i)}) \\\\\n",
    "& = \\sum_{k=1}^K \\sum_{j=1}^d \\underbrace{\\sum_{i :y^{(i)} =k} \\log P(x^{(i)}_j | \\psi_{jk})}_\\text{all the terms that involve $\\psi_{jk}$} + \\underbrace{\\sum_{i=1}^n \\log P(y^{(i)} | \\vec \\phi)}_\\text{all the terms that involve $\\vec \\phi$}\n",
    "\\end{align*}\n",
    "where, recall from Lecture 6 (derivation of cross-entropy) and Lecture 7 (derivation of the optimal solution of GDA), we have\n",
    "\\begin{align*}\n",
    "P(x^{(i)}_j | \\psi_{jk}) &= \\psi_{jk}^{x^{(i)}_j }(1-\\psi_{jk})^{1-x^{(i)}_j}, \\\\\n",
    "P(y^{(i)} | \\vec \\phi) &= \\frac{\\phi_{y^{(i)}}}{\\sum_{j = 1}^K\\phi_j}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Notice that each parameter $\\psi_{jk}$ is found in only and the $\\phi_k$ are also in the same set of terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "As in Gaussian Discriminant Analysis, the log-likelihood decomposes into a sum of terms. To optimize for some $\\psi_{jk}$, we only need to look at the set of terms that contain $\\psi_{jk}$:\n",
    "$$ I(\\psi_{jk}) = \\sum_{i :y^{(i)} =k}  \\log\\psi_{jk}^{x^{(i)}_j }(1-\\psi_{jk})^{1-x^{(i)}_j} =   \\sum_{i :y^{(i)} =k}  x^{(i)}_j\\log\\psi_{jk} + (1-x^{(i)}_j)\\log (1-\\psi_{jk}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Similarly, optimizing for $\\vec \\phi = (\\phi_1, \\phi_2, \\ldots, \\phi_K)$ only involves a single term:\n",
    "\\begin{align*}\n",
    "J(\\vec\\phi) & = \\sum_{i=1}^n  \\log P_\\theta(y^{(i)} | \\vec \\phi) \\\\\n",
    "& = \\sum_{i=1}^n \\log \\phi_{y^{(i)}} - n \\cdot \\log \\sum_{k=1}^K \\phi_k \\\\ \n",
    "& = \\sum_{k=1}^K \\sum_{i : y^{(i)} = k} \\log \\phi_k - n \\cdot \\log \\sum_{k=1}^K \\phi_k\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Optimizing the Model Parameters\n",
    "\n",
    "These observations greatly simplify the optimization of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "1. As in Gaussian Discriminant Analysis, taking the partial derivatives and setting them to zero, we obtain \n",
    "$$ \\frac{\\phi_k}{\\sum_l \\phi_l} = \\frac{n_k}{n}$$\n",
    "for each $k$, where $n_k = |\\{i : y^{(i)} = k\\}|$ is the number of training targets with class $k$. Thus, **the optimal $\\phi_k$ is just the proportion of data points with class $k$ in the training set**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "2. Similarly, setting the derivative of $I(\\psi_{jk})$ with respect to $\\psi_{jk}$ to zero gives\n",
    "\\begin{align*}\n",
    "\\psi_{jk} = \\frac{n_{jk}}{n_k}.\n",
    "\\end{align*}\n",
    "where $n_{jk} = |\\{i : x^{(i)}_j = 1 \\text{ and } y^{(i)} = k\\}|$ is the number of $x^{(i)}$ with label $k$ and a positive occurrence of word $j$. **The optimal $\\psi_{jk}$ is simply the proportion of documents in class $k$ that contain the word $j$**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Querying the Model\n",
    "\n",
    "How do we ask the model for predictions? As discussed earler, we can apply Bayes' rule:\n",
    "$$\\arg\\max_y P_\\theta(y|x) = \\arg\\max_y P_\\theta(x|y)P(y).$$\n",
    "Thus, we can estimate the probability of $x$ and under each $P_\\theta(x|y=k)P(y=k)$ and choose the class that explains the data best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Classification Dataset: Twenty Newsgroups\n",
    "\n",
    "To illustrate the text classification problem, we will use a popular dataset called `20-newsgroups`. \n",
    "* It contains ~20,000 documents collected approximately evenly from 20 different online newsgroups.\n",
    "* Each newgroup covers a different topic such as medicine, computer graphics, or religion.\n",
    "* This dataset is widely used to benchmark text classification and other types of algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Let's load this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _20newsgroups_dataset:\n",
      "\n",
      "The 20 newsgroups text dataset\n",
      "------------------------------\n",
      "\n",
      "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
      "20 topics split in two subsets: one for training (or development)\n",
      "and the other one for testing (or for performance evaluation). The split\n",
      "between the train and test set is based upon a messages posted before\n",
      "and after a specific date.\n",
      "\n",
      "This module contains two loaders. The first one,\n",
      ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
      "returns a list of the raw texts that can be fed to text feature\n",
      "extractors such as :class:`~sklearn.feature_extraction.text.CountVectorizer`\n",
      "with custom parameters so as to extract feature vectors.\n",
      "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
      "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
      "extractor.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   ==========\n",
      "    Classes                     20\n",
      "    Samples total            18846\n",
      "    Dimensionality               1\n",
      "    Features                  text\n",
      "    =================   ==========\n",
      "\n",
      "Usage\n",
      "~~~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# for this lecture, we will restrict our attention to just 4 different newsgroups:\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "\n",
    "# load the dataset\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "# print some information on it\n",
    "print(twenty_train.DESCR[:1100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Example: Text Classification\n",
    "\n",
    "Let's see how this approach can be used in practice on the text classification dataset.\n",
    "* We will learn a good set of parameters for a Bernoulli Naive Bayes model\n",
    "* We will compare the outputs to the true predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Let's see an example of this approach on `20-newsgroups`.\n",
    "\n",
    "We start by computing these features using the `sklearn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vectorize the training set\n",
    "count_vect = CountVectorizer(binary=True, max_features=1000)\n",
    "y_train = twenty_train.target\n",
    "X_train = count_vect.fit_transform(twenty_train.data).toarray()\n",
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Let's compute the maximum likelihood model parameters on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21267169 0.25875055 0.26318121 0.26539654]\n"
     ]
    }
   ],
   "source": [
    "# we can implement these formulas over the Iris dataset\n",
    "n = X_train.shape[0] # size of the dataset\n",
    "d = X_train.shape[1] # number of features in our dataset\n",
    "K = 4 # number of clases\n",
    "\n",
    "# these are the shapes of the parameters\n",
    "psis = np.zeros([K,d])\n",
    "phis = np.zeros([K])\n",
    "\n",
    "# we now compute the parameters\n",
    "for k in range(K):\n",
    "    X_k = X_train[y_train == k]\n",
    "    psis[k] = np.mean(X_k, axis=0)\n",
    "    phis[k] = X_k.shape[0] / float(n)\n",
    "\n",
    "# print out the class proportions\n",
    "print(phis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "We can compute predictions using Bayes' rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 3 0 3 3 3 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# we can implement this in numpy\n",
    "def nb_predictions(x, psis, phis):\n",
    "    \"\"\"This returns class assignments and scores under the NB model.\n",
    "    \n",
    "    We compute \\arg\\max_y p(y|x) as \\arg\\max_y p(x|y)p(y)\n",
    "    \"\"\"\n",
    "    # adjust shapes\n",
    "    n, d = x.shape\n",
    "    x = np.reshape(x, (1, n, d))\n",
    "    psis = np.reshape(psis, (K, 1, d))\n",
    "    \n",
    "    # clip probabilities to avoid log(0)\n",
    "    psis = psis.clip(1e-14, 1-1e-14)\n",
    "    \n",
    "    # compute log-probabilities\n",
    "    logpy = np.log(phis).reshape([K,1])\n",
    "    logpxy = x * np.log(psis) + (1-x) * np.log(1-psis)\n",
    "    logpyx = logpxy.sum(axis=2) + logpy\n",
    "\n",
    "    return logpyx.argmax(axis=0).flatten(), logpyx.reshape([K,n])\n",
    "\n",
    "idx, logpyx = nb_predictions(X_train, psis, phis)\n",
    "print(idx[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "We can measure the accuracy on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8692955250332299"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(idx==y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['OpenGL on the GPU is fast']\n",
    "\n",
    "X_new = count_vect.transform(docs_new).toarray()\n",
    "predicted, logpyx_new = nb_predictions(X_new, psis, phis)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Algorithm: Bernoulli Naive Bayes\n",
    "\n",
    "* __Type__: Supervised learning (multi-class classification)\n",
    "* __Model family__: Mixtures of Bernoulli distributions\n",
    "* __Objective function__: Log-likelihood.\n",
    "* __Optimizer__: Closed form solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Part 4: Another look at Discriminative vs. Generative Algorithms\n",
    "\n",
    "We conclude our lectures on generative algorithms by revisting the question of how they compare to discriminative algorithms. We shall make connection among Gaussian Discriminant Analysis, Gaussian Naive Bayes, and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "# Review: Generative Models\n",
    "\n",
    "There are two types of probabilistic models: *generative* and *discriminative*.\n",
    "\\begin{align*}\n",
    "\\underbrace{P_\\theta(x,y) : \\mathcal{X} \\times \\mathcal{Y} \\to [0,1]}_\\text{generative model} & \\;\\; & \\underbrace{P_\\theta(y|x) : \\mathcal{X} \\times \\mathcal{Y} \\to [0,1]}_\\text{discriminative model}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Given a new datapoint $x'$, we can match it against each class model and find the class that looks most similar to it:\n",
    "\\begin{align*}\n",
    "\\arg \\max_y \\log p(y | x) = \\arg \\max_y \\log \\frac{p(x | y) p(y)}{p(x)} = \\arg \\max_y \\log p(x | y) p(y),\n",
    "\\end{align*}\n",
    "where we have applied Bayes' rule in the second equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Review: Gaussian Discriminant Model\n",
    "\n",
    "The GDA algorithm defines the following model family.\n",
    "* The probability $P(x\\mid y=k)$ of the data under class $k$ is a [multivariate Gaussian](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) $\\mathcal{N}(x; \\mu_k, \\Sigma_k)$ with parameters\n",
    "$\\mu_k, \\Sigma_k$.\n",
    "* The distribution over classes is [Categorical](https://en.wikipedia.org/wiki/Categorical_distribution), denoted $\\text{Categorical}(\\phi_1, \\phi_2, ..., \\phi_K)$. Thus, $P_\\theta(y=k) = \\phi_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Thus, $P_\\theta(x)$ is a mixture of $K$ Gaussians:\n",
    "$$P_\\theta(x) = \\sum_{k=1}^K P_\\theta(y=k) P_\\theta(x|y=k) = \\sum_{k=1}^K \\phi_k \\mathcal{N}(x; \\mu_k, \\Sigma_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Linear Discriminant Analysis\n",
    "\n",
    "When the covariances $\\Sigma_k$ in GDA are equal, we have an algorithm called Linear Discriminant Analysis or LDA.\n",
    "\n",
    "Let's try this algorithm on the Iris flower dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "We may compute the parameters of this model similarly to how we did for GDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.006 3.428]\n",
      " [5.936 2.77 ]\n",
      " [6.588 2.974]]\n"
     ]
    }
   ],
   "source": [
    "# we can implement these formulas over the Iris dataset\n",
    "d = 2 # number of features in our toy dataset\n",
    "K = 3 # number of clases\n",
    "n = X.shape[0] # size of the dataset\n",
    "\n",
    "# these are the shapes of the parameters\n",
    "mus = np.zeros([K,d])\n",
    "Sigmas = np.zeros([K,d,d])\n",
    "phis = np.zeros([K])\n",
    "\n",
    "# we now compute the parameters\n",
    "for k in range(3):\n",
    "    X_k = X[iris_y == k]\n",
    "    mus[k] = np.mean(X_k, axis=0)\n",
    "    Sigmas[k] = np.cov(X.T) # this is now X.T instead of X_k.T\n",
    "    phis[k] = X_k.shape[0] / float(n)\n",
    "\n",
    "# print out the means\n",
    "print(mus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "We can compute predictions using Bayes' rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 2 2 2 1 2 1 2 1 2 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1\n",
      " 2 2 2 2 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 1 2 1 2 2\n",
      " 1 2 1 1 2 2 2 2 1 2 1 2 1 2 2 1 1 2 2 2 2 2 1 1 2 2 2 1 2 2 2 1 2 2 2 1 2\n",
      " 2 1]\n"
     ]
    }
   ],
   "source": [
    "# we can implement this in numpy\n",
    "def gda_predictions(x, mus, Sigmas, phis):\n",
    "    \"\"\"This returns class assignments and p(y|x) under the GDA model.\n",
    "    \n",
    "    We compute \\arg\\max_y p(y|x) as \\arg\\max_y p(x|y)p(y)\n",
    "    \"\"\"\n",
    "    # adjust shapes\n",
    "    n, d = x.shape\n",
    "    x = np.reshape(x, (1, n, d, 1))\n",
    "    mus = np.reshape(mus, (K, 1, d, 1))\n",
    "    Sigmas = np.reshape(Sigmas, (K, 1, d, d))    \n",
    "    \n",
    "    # compute probabilities\n",
    "    py = np.tile(phis.reshape((K,1)), (1,n)).reshape([K,n,1,1])\n",
    "    pxy = (\n",
    "        np.sqrt(np.abs((2*np.pi)**d*np.linalg.det(Sigmas))).reshape((K,1,1,1)) \n",
    "        * -.5*np.exp(\n",
    "            np.matmul(np.matmul((x-mus).transpose([0,1,3,2]), np.linalg.inv(Sigmas)), x-mus)\n",
    "        )\n",
    "    )\n",
    "    pyx = pxy * py\n",
    "    return pyx.argmax(axis=0).flatten(), pyx.reshape([K,n])\n",
    "\n",
    "idx, pyx = gda_predictions(X, mus, Sigmas, phis)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "We visualize predictions like we did earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.colors import LogNorm\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, .02), np.arange(y_min, y_max, .02))\n",
    "Z, pyx = gda_predictions(np.c_[xx.ravel(), yy.ravel()], mus, Sigmas, phis)\n",
    "logpy = np.log(-1./3*pyx)\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "contours = np.zeros([K, xx.shape[0], xx.shape[1]])\n",
    "for k in range(K):\n",
    "    contours[k] = logpy[k].reshape(xx.shape)\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=iris_y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Linear Discriminant Analysis outputs decision boundaries that are linear.\n",
    "\n",
    "Softmax or Logistic regression also produce linear boundaries. In fact, both types of algorithms make use of the same model class.\n",
    "\n",
    "What is their difference then?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Generative vs. Discriminative Model Classes\n",
    "\n",
    "In binary classification, we can also show that the conditional probability $P_\\theta(y|x)$ of a Gaussian Naive Bayes (**replacing Bernoulli with Gaussian when $x$ is continuous**) or LDA model has the form\n",
    "$$ P_\\theta(y|x) = \\frac{P_\\theta(x|y)P_\\theta(y)}{\\sum_{y'\\in \\mathcal{Y}}P_\\theta(x|y')P_\\theta(y')} = \\frac{1}{1+\\exp(-\\gamma^\\top x)} $$\n",
    "for some set of parameters $\\gamma$ (whose expression can be derived from $\\theta$), which is the same form as Logistic Regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Does it mean that the two sets of algorithms are equivalent?** No! They assume the same model class $\\mathcal{M}$, they use a different objective $J$ to select a model in $\\mathcal{M}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Generative Models vs. Logistic Regression\n",
    "\n",
    "Given that both algorithms find linear boundaries, how should one choose between the two?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Gaussian Naive Bayes (GNB) or LDA assumes a logistic form for $p(y|x)$. But converse is not true: logistic regression does not assume a GNB or LDA model for $p(x,y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Generative models make stronger modeling assumptions. If these assumptions hold true, the generative models will perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* But if they don't, logistic regression will be more robust to outliers and model misspecification, and achieve higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Other Features of Generative Models\n",
    "\n",
    "Generative models can also do things that discriminative models can't do.\n",
    "* Generation: we can sample $x \\sim p(x|y)$ to generate new data (images, audio).\n",
    "* Missing value imputation: if $x_j$ is missing, we can infer it using $p(x|y)$.\n",
    "* Outlier detection: given a new $x'$, we can try detecting via $p(x')$ if $x'$ is invalid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Discriminative Approaches\n",
    "\n",
    "Discriminative algorithms are deservingly very popular.\n",
    "* Most state-of-the-art algorithms for classification are discriminative\n",
    "* They are often more accurate because they make fewer modeling assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "# Generative Approaches\n",
    "\n",
    "But generative algorithms also have many advantages:\n",
    "* Can do more than just prediction: generation, fill-in missing features, etc.\n",
    "* Can include extra prior knowledge; if prior knowledge is correct, model will be more accurate.\n",
    "* Often have closed-form solutions, hence are faster to train."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "neural-ode.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "rise": {
   "controlsTutorial": false,
   "height": 900,
   "help": false,
   "margin": 0,
   "maxScale": 2,
   "minScale": 0.2,
   "progress": true,
   "scroll": true,
   "theme": "simple",
   "width": 1200
  },
  "vscode": {
   "interpreter": {
    "hash": "21be62648f8e3839c3b4ce05d43053c0ccba5ecc90dec2be15f843391ed2568c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
